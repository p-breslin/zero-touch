import os
import json
import asyncio
import logging
from pathlib import Path
from typing import Any, Dict, List, Coroutine, AsyncIterator, Sequence

from dotenv import load_dotenv

from agno.workflow import Workflow
from agno.agent import Agent, RunResponse
from agno.knowledge.agent import AgentKnowledge
from agno.embedder.openai import OpenAIEmbedder
from agno.vectordb.chroma.chromadb import ChromaDb

from src.paths import DATA_DIR
from agents.agent_builder import build_agent
from utils.logging_setup import setup_logging
from services.create_engine import create_db_engine
from utils.helpers import validate_response, load_yaml
from models import (
    KBInfo,
    SQLPlan,
    SQLQueries,
    SQLQuery,
    SingleTableResult,
    SQLResults,
    AggregatedData,
    IdentityList,
)

load_dotenv()
setup_logging()
log = logging.getLogger(__name__)

# ------------------------------------------------------------------------------
# Helper utilities
# ------------------------------------------------------------------------------


def _parse_response(response: RunResponse | None, model: Any, agent_name: str):
    if not response or not response.content:
        raise ValueError(f"{agent_name} returned no content")
    return validate_response(response.content, model, savefile=agent_name)


# ------------------------------------------------------------------------------
# Main workflow
# ------------------------------------------------------------------------------


class Pipeline(Workflow):
    """Identity-inference orchestrator with leaner plumbing."""

    def __init__(self, **kwargs: Any):
        super().__init__(**kwargs)
        self.db_engine = create_db_engine()
        log.info("Pipeline ready - session %s", self.session_id)

    def _agent(self, key: str, *, with_kb: bool = False) -> Agent:
        return build_agent(
            agent_key=key,
            db_engine=self.db_engine,
            knowledge_base=self.knowledge_base if with_kb else None,
        )

    def add_to_session(self):
        output_path = str(DATA_DIR / "output")

        # 1) Knowledge Base Agent ----------------------------------------------
        log.info("Loading output from KnowledgeBase_Agent...")
        with open(f"{output_path}/KnowledgeBase_Agent.json", "r") as f:
            schema_info = json.load(f)
        self.session_state["knowledgebase_info"] = json.dumps(schema_info, indent=2)

        # 2) Planner Agent -----------------------------------------------------
        log.info("Loading output from Planner_Agent...")
        with open(f"{output_path}/Planner_Agent.json", "r") as f:
            sql_plan = json.load(f)
        self.session_state["sql_plan"] = json.dumps(sql_plan, indent=2)

        # 3) SQL Constructor Agent ---------------------------------------------
        log.info("Loading output from SQL_Constructor_Agent...")
        with open(f"{output_path}/SQL_Constructor_Agent.json", "r") as f:
            sql_queries = json.load(f)
        self.session_state["sql_queries"] = json.dumps(sql_queries, indent=2)

        # 4+5) SQL Aggregated Results ------------------------------------------
        log.info("Loading output from Data_Aggregation...")
        with open(f"{output_path}/Data_Aggregation.json", "r") as f:
            agg_data = json.load(f)
        self.session_state["aggregated_data"] = json.dumps(agg_data, indent=2)

    # --------------------------------------------------------------------------
    # Workflow entryâ€‘point
    # --------------------------------------------------------------------------

    async def arun(self) -> AsyncIterator[RunResponse]:
        # Load queries and saved outputs
        queries = load_yaml("queries")
        self.add_to_session()

        # 6. Identity Inference Agent ------------------------------------------
        identity_query = queries["identity_query"].format(
            aggregated_data_json=self.session_state["aggregated_data"]
        )

        identity_agent = self._agent("Identity_Agent")
        identity_resp = await identity_agent.arun(identity_query)
        try:
            identities = _parse_response(
                identity_resp, IdentityList, identity_agent.name
            )
        except Exception:
            log.error("Skipping run: Identity list validation failed")
            return
        self.session_state["identities"] = identities.model_dump()

        log.info(f"IdentityList generated by {identity_agent.name}")
        yield RunResponse(
            event="Identity_Inference_complete",
            content="Identity resolution complete.",
            run_id=self.run_id,
            session_id=self.session_id,
        )


# ------------------------------------------------------------------------------
# Runner
# ------------------------------------------------------------------------------

if __name__ == "__main__":
    from agno.storage.sqlite import SqliteStorage
    from agno.utils.pprint import pprint_run_response

    storage = SqliteStorage(
        table_name="orchestrator_sessions",
        db_file=str(DATA_DIR / "orchestrator_main_session_storage.db"),
        auto_upgrade_schema=True,
    )

    pipeline = Pipeline(
        name="MainOrchestrator",
        session_id="fixed_test_session_id",
        storage=storage,
        debug_mode=True,
    )

    async def _main():
        chunks = []
        final_event_reached = "workflow_started"
        async for chunk in pipeline.arun():
            chunks.append(chunk)
            pprint_run_response(chunk, markdown=False)
            final_event_reached = chunk.event

            if "failed" in (chunk.event or "").lower():
                log.error(f"Workflow failed after: {final_event_reached}")
                break

            elif final_event_reached == "Identity_Inference_complete":
                log.info("Workflow test complete.")

            else:
                log.warning(
                    f"Workflow completed but last event was: {final_event_reached}"
                )

    asyncio.run(_main())
